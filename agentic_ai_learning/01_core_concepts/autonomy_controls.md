# Autonomy Controls

## What It Is

- **Autonomy controls** = mechanisms that constrain what an agent can do, when, and how much
- The spectrum from "fully supervised" to "fully autonomous"
- Includes guardrails, human-in-the-loop (HITL), safety layers, and ethical constraints
- The goal: **maximize agent capability while minimizing risk**

## Why It Matters (Interview Framing)

> "Any engineer can build an agent that works in a demo. The interview question is: how do you make it safe for production? Autonomy controls are the difference between a cool prototype and a deployable system."

---

## Simple Mental Model

> Think of autonomy controls like **driver assistance levels:**
>
> | Level | Driving Analogy | Agent Analogy |
> |---|---|---|
> | L0 | Manual driving | User does everything, LLM just suggests |
> | L1 | Cruise control | Agent executes simple tasks, user approves |
> | L2 | Lane assist + cruise | Agent handles routine tasks, escalates edge cases |
> | L3 | Conditional autonomy | Agent works independently in defined scope |
> | L4 | Full self-driving (limited area) | Agent fully autonomous in constrained domain |
> | L5 | Full autonomy anywhere | Fully autonomous agent (not recommended today) |

üí° **Most production agents are L2-L3.** L5 is aspirational, not practical.

---

## The Four Control Mechanisms

### 1. Guardrails

```
User Input ‚Üí [Input Guardrail] ‚Üí Agent ‚Üí [Output Guardrail] ‚Üí Response
                  ‚îÇ                              ‚îÇ
                  ‚ñº                              ‚ñº
            Block/modify                   Block/modify
            harmful input                  harmful output
```

**Types of guardrails:**

| Guardrail | What It Does | Example |
|---|---|---|
| **Input validation** | Block harmful/out-of-scope prompts | Reject prompt injection attempts |
| **Output filtering** | Block harmful/incorrect responses | Filter PII, profanity, hallucinations |
| **Tool restrictions** | Limit which tools agent can call | No DELETE operations, read-only DB access |
| **Budget limits** | Cap token/API/cost spend | Max $0.50 per agent run |
| **Scope boundaries** | Restrict agent to specific domains | Customer support agent can't access HR data |

```python
# Example: Guardrail configuration
guardrails = {
    "max_tokens_per_run": 50000,
    "max_tool_calls": 20,
    "max_cost_usd": 1.00,
    "allowed_tools": ["search_kb", "get_order", "send_email"],
    "blocked_actions": ["delete_*", "admin_*"],
    "pii_filter": True,
    "content_filter": True
}
```

---

### 2. Human-in-the-Loop (HITL)

```
Agent works autonomously
    ‚îÇ
    ‚îú‚îÄ‚îÄ Low-risk action ‚Üí Execute automatically
    ‚îÇ
    ‚îú‚îÄ‚îÄ Medium-risk action ‚Üí Notify human, continue
    ‚îÇ
    ‚îî‚îÄ‚îÄ High-risk action ‚Üí PAUSE, wait for human approval
                              ‚îÇ
                              ‚îú‚îÄ‚îÄ Human approves ‚Üí Continue
                              ‚îî‚îÄ‚îÄ Human rejects ‚Üí Abort or modify
```

**HITL patterns:**

| Pattern | Description | Use Case |
|---|---|---|
| **Approval gates** | Agent pauses for human approval before critical actions | Financial transactions, data deletion |
| **Escalation** | Agent hands off to human when confidence is low | Customer support edge cases |
| **Audit trail** | Human reviews after-the-fact | Compliance, regulated industries |
| **Collaborative** | Human and agent work together in real-time | Code review, document editing |

üí° **HITL is not a failure of the agent ‚Äî it's a design feature.** The best agents know when to ask for help.

---

### 3. Safety Layers

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ           SAFETY STACK                 ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ  Layer 4: Business Rules              ‚îÇ
‚îÇ   "Never offer refunds > $500"        ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ  Layer 3: Content Safety              ‚îÇ
‚îÇ   PII detection, toxicity filter      ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ  Layer 2: Tool Safety                 ‚îÇ
‚îÇ   Permission checks, rate limits      ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ  Layer 1: Prompt Safety               ‚îÇ
‚îÇ   Injection detection, jailbreak      ‚îÇ
‚îÇ   detection                           ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

- Safety is **defense in depth** ‚Äî multiple layers, each catches different threats
- No single layer is sufficient on its own
- Each layer should fail independently

---

### 4. Ethical Constraints

- **Fairness:** Agent doesn't discriminate based on protected attributes
- **Transparency:** Agent explains its reasoning when asked
- **Privacy:** Agent respects data minimization and retention policies
- **Accountability:** Every action is logged and attributable
- **Harm prevention:** Agent refuses requests that could cause harm

---

## Designing Autonomy Levels

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ         AUTONOMY DECISION MATRIX                 ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ          ‚îÇ Low Risk ‚îÇ Med Risk ‚îÇ High Risk      ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇHigh Conf ‚îÇ Auto     ‚îÇ Auto +   ‚îÇ Approval gate  ‚îÇ
‚îÇ          ‚îÇ execute  ‚îÇ notify   ‚îÇ                ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇMed Conf  ‚îÇ Auto +   ‚îÇ Approval ‚îÇ Escalate to    ‚îÇ
‚îÇ          ‚îÇ log      ‚îÇ gate     ‚îÇ human          ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇLow Conf  ‚îÇ Approval ‚îÇ Escalate ‚îÇ BLOCK          ‚îÇ
‚îÇ          ‚îÇ gate     ‚îÇ to human ‚îÇ                ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

- **Confidence** = how sure the agent is about its decision
- **Risk** = potential impact if the action is wrong
- Combine both to determine the right autonomy level

---

## Practical Example: Financial Agent Controls

```python
class FinancialAgentControls:
    def check_action(self, action):
        # Layer 1: Scope check
        if action.type not in ALLOWED_ACTIONS:
            return Block("Action not in scope")

        # Layer 2: Amount check
        if action.amount > 500:
            return RequireApproval(
                reason=f"Transaction ${action.amount} exceeds $500 limit"
            )

        # Layer 3: Anomaly check
        if self.is_anomalous(action):
            return Escalate(
                reason="Unusual pattern detected",
                to="risk_team"
            )

        # Layer 4: Rate limit
        if self.daily_total + action.amount > 10000:
            return Block("Daily limit exceeded")

        return Approve()
```

---

## Interview Questions They Will Ask

1. **"How do you decide how much autonomy to give an agent?"**
   ‚Üí Risk-based matrix: combine action risk level with agent confidence. High risk + low confidence = human approval required.

2. **"What guardrails would you put on a production agent?"**
   ‚Üí Input validation, output filtering, tool restrictions, budget caps, PII detection, scope boundaries, rate limits.

3. **"How do you implement human-in-the-loop?"**
   ‚Üí Approval gates for high-risk actions. Escalation paths for low-confidence situations. Async approval via Slack/email for non-blocking flows.

4. **"What happens when an agent tries to do something it shouldn't?"**
   ‚Üí Defense in depth: the guardrail closest to the action blocks it. Log the attempt. Alert the team. The agent receives an error and re-plans.

5. **"How do you balance autonomy with safety?"**
   ‚Üí Start restrictive (L1-L2), measure failure rates, gradually increase autonomy. Always keep a human escalation path. Never go full L5 in regulated domains.

---

## Common Mistakes

‚ö†Ô∏è **No guardrails at all** ‚Äî "It works in the demo" is not a production safety strategy.

‚ö†Ô∏è **Binary thinking** ‚Äî It's not "autonomous" vs "not autonomous." Design a spectrum with appropriate gates at each level.

‚ö†Ô∏è **Guardrails only on output** ‚Äî You need input validation too. Prompt injection is real.

‚ö†Ô∏è **HITL that blocks everything** ‚Äî If every action needs approval, the agent is useless. Reserve HITL for genuinely high-risk actions.

‚ö†Ô∏è **No logging** ‚Äî If you can't audit what the agent did and why, you can't debug, improve, or comply with regulations.

---

## TL;DR

- **Autonomy is a spectrum** (L0-L5) ‚Äî most production agents are L2-L3
- Four control mechanisms: **Guardrails, HITL, Safety Layers, Ethical Constraints**
- Use a **risk √ó confidence matrix** to set autonomy levels per action
- **Defense in depth** ‚Äî multiple safety layers, each independent
- Start restrictive, **gradually increase autonomy** based on measured reliability
