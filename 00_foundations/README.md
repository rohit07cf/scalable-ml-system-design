# Foundations

## What This Section Covers

- Core LLM architecture, tokenization, and inference mechanics
- Latency, throughput, and cost reasoning for ML systems
- Evaluation fundamentals: metrics, offline vs online, pitfalls
- Reliability and orchestration patterns for production ML


## What Interviewers Usually Test

- Can you reason about latency vs cost without hand-waving?
- Do you understand how LLMs actually work under the hood?
- Can you pick the right evaluation strategy for a given problem?


## Suggested Study Order

1. llm_fundamentals
2. latency_cost_tradeoffs
3. evaluation_basics
4. reliability_patterns
5. temporal_orchestration_patterns


## Fast Revision Path

- Re-read "must explain in interview" bullets in each subfolder
- Sketch a latency budget for a two-model pipeline from memory
- Walk through one reliability pattern end-to-end (e.g., circuit breaker)
